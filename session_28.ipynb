{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train ResNet10 on CIFAR10 dataset. In the first code, train without any optimization. Then implement all of your strategies such that you achieve 10x speed up! You're free to use/do anything. You need to run only for 5 epochs each.\n",
        "\n",
        "Once done, upload a screenshot of your 10 logs from unoptimized and optimized runs. Share the GitHub link where I can see the logs as well.\n",
        "\n"
      ],
      "metadata": {
        "id": "732RXgD-pCXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G8bSmKbGpBfz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from torchsummary import summary\n",
        "except ModuleNotFoundError:\n",
        "    !pip install torchsummary\n",
        "    from torchsummary import summary\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                      #  transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.RandomAffine(degrees=10, shear = 10),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "train = datasets.CIFAR10(root = './data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "\n",
        "# Do we have CUDA drivers for us?\n",
        "cuda = torch.cuda.is_available()\n",
        "print (\"Cuda Available?\", cuda)\n",
        "\n",
        "dataloader_args = dict(shuffle=True, batch_size=2048, num_workers=2, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train, **dataloader_args)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, **dataloader_args)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_yxR2s92RqM",
        "outputId": "bb822c6e-e0c4-4401-ed83-cf1778bc673e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Cuda Available? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv01 = nn.Conv2d(3, 16, 3, bias=False, padding=1)\n",
        "        self.batch01 = nn.BatchNorm2d(num_features=16)\n",
        "\n",
        "        # ---- Lets take a skip connection\n",
        "        self.skip_conv1 = nn.Conv2d(16, 16, 3, padding=0, dilation=2)\n",
        "\n",
        "        self.conv02 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n",
        "        self.batch02 = nn.BatchNorm2d(num_features=16)\n",
        "        self.conv03 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n",
        "        self.batch03 = nn.BatchNorm2d(num_features=16)\n",
        "        self.conv04 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n",
        "        self.batch04 = nn.BatchNorm2d(num_features=16)\n",
        "        self.pool01 = nn.MaxPool2d(2, 2)                                #O=16\n",
        "        self.conv05 = nn.Conv2d(16, 16, 1, bias=False)\n",
        "\n",
        "        self.conv11 = nn.Conv2d(16, 32, 3, bias=False, padding=1)\n",
        "        self.batch11 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv12 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n",
        "        self.batch12 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv13 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n",
        "        self.batch13 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv14 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n",
        "        self.batch14 = nn.BatchNorm2d(num_features=32)\n",
        "        self.pool11 = nn.MaxPool2d(2, 2)                                #O=8\n",
        "        self.conv15 = nn.Conv2d(32, 32, 1, bias=False)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(32, 64, 3, bias=False, padding=1)\n",
        "        self.batch21 = nn.BatchNorm2d(num_features=64)\n",
        "        self.conv22 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n",
        "        self.batch22 = nn.BatchNorm2d(num_features=64)\n",
        "        self.conv23 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n",
        "        self.batch23 = nn.BatchNorm2d(num_features=64)\n",
        "        self.conv24 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n",
        "        self.batch24 = nn.BatchNorm2d(num_features=64)\n",
        "        self.pool21 = nn.MaxPool2d(2, 2)                                #O=4\n",
        "        self.conv25 = nn.Conv2d(64, 64, 1, bias=False)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, groups=64, bias = False, padding = 1)\n",
        "        self.convPV1= nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, bias = False, padding = 0)\n",
        "        self.batch31 = nn.BatchNorm2d(num_features=128)\n",
        "        self.conv32 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n",
        "        self.convPV2= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, bias = False, padding = 0)\n",
        "        self.batch32 = nn.BatchNorm2d(num_features=256)\n",
        "\n",
        "\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n",
        "        self.convx3 = nn.Conv2d(256, 10, 1, bias=False, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batch01(F.relu(self.conv01(x)))\n",
        "\n",
        "        # ---- Lets take a skip connection\n",
        "        skip_channels = self.skip_conv1(self.skip_conv1(self.skip_conv1(self.skip_conv1(x))))\n",
        "\n",
        "        x = self.batch02(F.relu(self.conv02(x)))\n",
        "        x = self.batch03(F.relu(self.conv03(x)))\n",
        "        x = self.batch04(F.relu(self.conv04(x)))\n",
        "        x = self.pool01(x)\n",
        "        x = self.conv05(x)\n",
        "        # ----------------------------------------------------------\n",
        "\n",
        "        # ---- Lets add the skip connection here\n",
        "        x = skip_channels + x\n",
        "\n",
        "        x = self.batch11(F.relu(self.conv11(x)))\n",
        "        x = self.batch12(F.relu(self.conv12(x)))\n",
        "        x = self.batch13(F.relu(self.conv13(x)))\n",
        "        x = self.batch14(F.relu(self.conv14(x)))\n",
        "        x = self.pool11(x)\n",
        "        x = self.conv15(x)\n",
        "        # ----------------------------------------------------------\n",
        "\n",
        "        x = self.batch21(F.relu(self.conv21(x)))\n",
        "        x = self.batch22(F.relu(self.conv22(x)))\n",
        "        x = self.batch23(F.relu(self.conv23(x)))\n",
        "        x = self.batch24(F.relu(self.conv24(x)))\n",
        "        x = self.pool21(x)\n",
        "        x = self.conv25(x)\n",
        "        # ----------------------------------------------------------\n",
        "\n",
        "        x = self.batch31(F.relu(self.convPV1(F.relu(self.conv31(x)))))\n",
        "        x = self.batch32(F.relu(self.convPV2(F.relu(self.conv32(x)))))\n",
        "\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.convx3(x)\n",
        "        x = x.view(-1, 10)                           # Don't want 10x1x1..\n",
        "        return F.log_softmax(x, dim=1)  # Added dim=1 parameter)"
      ],
      "metadata": {
        "id": "K10_ILasJFly"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZsEKXnM7Iao",
        "outputId": "68ec486c-9833-4487-a9bd-a1dec633c403"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 28, 28]           2,320\n",
            "            Conv2d-4           [-1, 16, 24, 24]           2,320\n",
            "            Conv2d-5           [-1, 16, 20, 20]           2,320\n",
            "            Conv2d-6           [-1, 16, 16, 16]           2,320\n",
            "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-10           [-1, 16, 32, 32]              32\n",
            "           Conv2d-11           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
            "        MaxPool2d-13           [-1, 16, 16, 16]               0\n",
            "           Conv2d-14           [-1, 16, 16, 16]             256\n",
            "           Conv2d-15           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-16           [-1, 32, 16, 16]              64\n",
            "           Conv2d-17           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-18           [-1, 32, 16, 16]              64\n",
            "           Conv2d-19           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-20           [-1, 32, 16, 16]              64\n",
            "           Conv2d-21           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-22           [-1, 32, 16, 16]              64\n",
            "        MaxPool2d-23             [-1, 32, 8, 8]               0\n",
            "           Conv2d-24             [-1, 32, 8, 8]           1,024\n",
            "           Conv2d-25             [-1, 64, 8, 8]          18,432\n",
            "      BatchNorm2d-26             [-1, 64, 8, 8]             128\n",
            "           Conv2d-27             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
            "           Conv2d-29             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-30             [-1, 64, 8, 8]             128\n",
            "           Conv2d-31             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-32             [-1, 64, 8, 8]             128\n",
            "        MaxPool2d-33             [-1, 64, 4, 4]               0\n",
            "           Conv2d-34             [-1, 64, 4, 4]           4,096\n",
            "           Conv2d-35             [-1, 64, 4, 4]             576\n",
            "           Conv2d-36            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-37            [-1, 128, 4, 4]             256\n",
            "           Conv2d-38            [-1, 128, 4, 4]           1,152\n",
            "           Conv2d-39            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-40            [-1, 256, 4, 4]             512\n",
            "        AvgPool2d-41            [-1, 256, 1, 1]               0\n",
            "           Conv2d-42             [-1, 10, 1, 1]           2,560\n",
            "================================================================\n",
            "Total params: 230,192\n",
            "Trainable params: 230,192\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.22\n",
            "Params size (MB): 0.88\n",
            "Estimated Total Size (MB): 3.11\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "time_taken = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    time_taken.clear()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate (pbar):\n",
        "        t0 = time.time()\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        #Don't want history of gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_predict = model(data)\n",
        "\n",
        "        # Caluclate loss\n",
        "        loss = F.nll_loss(y_predict, target)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        # Back propogate error\n",
        "        loss.backward()\n",
        "\n",
        "        # Take a optimzer step\n",
        "        optimizer.step()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "\n",
        "        time_taken.append((t1-t0))\n",
        "\n",
        "        pred = y_predict.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        # print(f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f} Time taken per iter = {dt :.2f}ms')\n",
        "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "        train_acc.append(100*correct/processed)\n",
        "\n",
        "\n",
        "def test (model, device, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "\n",
        "model =  Net().to(device)\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    print(f\" --> EPOCH: {epoch}, Avg Time Taken = {(sum(time_taken)/len(time_taken))*1000:.2f}ms\")\n",
        "    # scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "MMVJZShF7YfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1428628d-ca59-473b-822a-2f2c2134c340"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.8440531492233276 Batch_id=24 Accuracy=20.10: 100%|██████████| 25/25 [00:22<00:00,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 0, Avg Time Taken = 364.24ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3147, Accuracy: 1018/10000 (10.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.5698350667953491 Batch_id=24 Accuracy=36.91: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 1, Avg Time Taken = 345.98ms\n",
            "\n",
            "Test set: Average loss: 1.6351, Accuracy: 4002/10000 (40.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.3989874124526978 Batch_id=24 Accuracy=45.71: 100%|██████████| 25/25 [00:16<00:00,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 2, Avg Time Taken = 351.46ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.4153, Accuracy: 4768/10000 (47.68%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2505789995193481 Batch_id=24 Accuracy=51.70: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 3, Avg Time Taken = 353.29ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.3094, Accuracy: 5279/10000 (52.79%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1860575675964355 Batch_id=24 Accuracy=56.12: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 4, Avg Time Taken = 349.37ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2343, Accuracy: 5523/10000 (55.23%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def check_bf16_support():\n",
        "    if torch.cuda.is_available():\n",
        "        compute_capability = torch.cuda.get_device_capability()\n",
        "        # Ampere (8.x) and newer GPUs support BF16\n",
        "        return compute_capability[0] >= 8\n",
        "    return False\n",
        "\n",
        "check_bf16_support()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSw1HmpymZoZ",
        "outputId": "fec987be-b4ea-4d1c-c430-bcb15f5aada7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BF16 is not available, Lets go to FP16"
      ],
      "metadata": {
        "id": "I0V_oJxcoX34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast, GradScaler  # Updated import\n",
        "torch.backends.cudnn.benchmark = True\n",
        "n_epochs = 5\n",
        "\n",
        "# 1. Setup model and optimizer with FP16 support\n",
        "def setup_fp16_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.compile(Net().to(device))\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=n_epochs)\n",
        "    # Updated GradScaler initialization\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    return model, optimizer, scaler, device\n",
        "\n",
        "# 2. Modified training loop\n",
        "def train(model, device, train_loader, optimizer, scaler, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    time_taken.clear()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Convert data to FP16 before moving to GPU\n",
        "        data = data.half()  # Convert to FP16\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Updated autocast\n",
        "        with autocast('cuda', dtype=torch.float16):\n",
        "            y_predict = model(data)\n",
        "            loss = F.nll_loss(y_predict, target)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # scheduler.step()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "\n",
        "        time_taken.append((t1-t0))\n",
        "\n",
        "        pred = y_predict.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc=f'Loss={loss.item()} Batch_idx={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "        train_acc.append(100*correct/processed)\n",
        "\n",
        "# 3. Modified test loop\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Convert data to FP16 before moving to GPU\n",
        "            data = data.half()  # Convert to FP16\n",
        "\n",
        "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "\n",
        "            # Updated autocast\n",
        "            with autocast('cuda'):\n",
        "                output = model(data)\n",
        "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "# 4. Training setup and execution\n",
        "model, optimizer, scaler, device = setup_fp16_model()\n",
        "\n",
        "# Your training loop\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    # print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, scaler, epoch)\n",
        "    print(f\" --> EPOCH: {epoch}, Avg Time Taken = {(sum(time_taken)/len(time_taken))*1000:.2f}ms\")\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TVLe-WnocLm",
        "outputId": "a9a05c97-4ed4-4820-d863-03498c1ea5f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.9019148349761963 Batch_idx=24 Accuracy=19.70: 100%|██████████| 25/25 [00:44<00:00,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 1, Avg Time Taken = 1375.91ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3237, Accuracy: 1000/10000 (10.00%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.487034797668457 Batch_idx=24 Accuracy=37.28: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 2, Avg Time Taken = 141.22ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.5923, Accuracy: 4166/10000 (41.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.409718632698059 Batch_idx=24 Accuracy=45.82: 100%|██████████| 25/25 [00:16<00:00,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 3, Avg Time Taken = 140.07ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.4568, Accuracy: 4739/10000 (47.39%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.314367651939392 Batch_idx=24 Accuracy=51.11: 100%|██████████| 25/25 [00:16<00:00,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 4, Avg Time Taken = 140.72ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.3476, Accuracy: 5141/10000 (51.41%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1803094148635864 Batch_idx=24 Accuracy=54.78: 100%|██████████| 25/25 [00:16<00:00,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> EPOCH: 5, Avg Time Taken = 140.00ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2401, Accuracy: 5487/10000 (54.87%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}